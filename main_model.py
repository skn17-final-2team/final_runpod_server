import os, time, torch, platform, re, json 
from dotenv import load_dotenv
from huggingface_hub import login

from peft import PeftModel
from langchain.vectorstores import FAISS
from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.embeddings import HuggingFaceEmbeddings

from typing import Any, List, Optional
from langchain_core.language_models.llms import LLM
from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.prompt_values import PromptValue
from langchain_core.messages import BaseMessage


os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"

# ===== ëª¨ë¸ ì„¤ì • =====
base_model_name = "Qwen/Qwen2.5-1.5B-Instruct"
ft_model_name = "CHOROROK/Qwen2.5_1.5B_trained_model_v3"

# ===== ì´ìŠ¤ì¼€ì´íŠ¸ =====
def escape_curly(text: str) -> str:
    return text.replace("{", "{{").replace("}", "}}")


# ===== ë²¡í„°DB ë¡œë“œ =====
def load_faiss_db(db_path: str):
    embedding_model = HuggingFaceEmbeddings(model_name="dragonkue/snowflake-arctic-embed-l-v2.0-ko")
    vector_store = FAISS.load_local(db_path, embedding_model, allow_dangerous_deserialization=True)
    print("ğŸ”µ FAISS DB ë¡œë“œ ì™„ë£Œ!\n")
    return vector_store, embedding_model


# ===== í˜•ì‹ì§€ì • =====
class HFTextGenLLM(LLM):
    """HF text-generation pipelineì„ ê°ì‹¸ëŠ”, ë¹„-ìŠ¤íŠ¸ë¦¬ë° LLM ë˜í¼."""
    pipe: Any
    model_config = {"arbitrary_types_allowed": True}

    @property
    def _llm_type(self) -> str:
        return "hf_text_generation_pipeline"

    def _normalize_prompt(self, prompt) -> str:
        """strë¡œ ì •ê·œí™”."""
        # PromptValue (PromptTemplate | ChatPromptTemplate ê²°ê³¼)
        if isinstance(prompt, PromptValue):
            return prompt.to_string()

        # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        if isinstance(prompt, list) and prompt:
            if isinstance(prompt[0], BaseMessage):
                return "\n".join(m.content for m in prompt)

        # ì´ë¯¸ ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ
        if isinstance(prompt, str):
            return prompt

        # 4) ë‚˜ë¨¸ì§€ëŠ” ê·¸ëƒ¥ ë¬¸ìì—´ ìºìŠ¤íŒ…
        return str(prompt)

    def _call(
        self,
        prompt,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs,
    ) -> str:
        text = self._normalize_prompt(prompt)

        # HF text-generation pipeline ì‹¤í–‰
        outputs = self.pipe(text)

        # transformers pipeline("text-generation") ê¸°ë³¸ ë°˜í™˜ í˜•ì‹: [{"generated_text": "..."}]
        if not outputs:
            return ""

        first = outputs[0]
        generated = first.get("generated_text") or first.get("text") or ""

        if stop:
            for s in stop:
                if s in generated:
                    generated = generated.split(s)[0]
                    break

        return generated


# ===== ëª¨ë¸ ë¡œë“œ =====
def load_model_q(model_name: str | None = base_model_name , adapter_name: str | None = ft_model_name):
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    if platform.system() == "Windows":
        print("âš  Windowsì—ì„œëŠ” 4bit ë¶ˆê°€ â†’ FP16ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map='auto'
        )
    else:
        print("ğŸ”µ Linux/RunPod í™˜ê²½: 4bit ì—†ì´ bf16ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.bfloat16,   
            device_map="auto",
        )

    if adapter_name:
        print(f"ğŸ”µ LoRA/PEFT ì–´ëŒ‘í„° ë¡œë“œ: {adapter_name}")
        model = PeftModel.from_pretrained(base_model, adapter_name)
    else:
        model = base_model

    text_gen_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        return_full_text=False, 
        max_new_tokens = 2048, 
        temperature=0.2,
        top_p=0.9
    )

    llm = HFTextGenLLM(pipe = text_gen_pipe)
    return llm


# ===== ë„ë©”ì¸ í•„í„° =====
def make_filter(filter: dict):
    if any(filter.values()):
        main_filter = filter.copy()
    else:
        main_filter = None
    return main_filter