import os, time, torch, platform, re, json 
from dotenv import load_dotenv
from huggingface_hub import login

from langchain.vectorstores import FAISS
from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace

from langchain.prompts import ChatPromptTemplate
from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser

from langchain.tools import tool
from langchain.agents import Tool
from langchain.agents import create_react_agent, AgentExecutor
from langchain.prompts import MessagesPlaceholder

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"

# local_path = "/workspace/snowflake-arctic-embed-l-v2.0-ko"

# from sentence_transformers import SentenceTransformer
# model = SentenceTransformer(local_path)

# ===== ë²¡í„° DB ë¡œë“œ =====
def load_faiss_db(db_path: str):
    embedding_model = HuggingFaceEmbeddings(model_name="dragonkue/snowflake-arctic-embed-l-v2.0-ko")
    vector_store = FAISS.load_local(db_path, embedding_model, allow_dangerous_deserialization=True)
    print("ğŸ”µ FAISS DB ë¡œë“œ ì™„ë£Œ!\n")
    return vector_store, embedding_model


# ===== ëª¨ë¸ ë¡œë“œ =====
def load_model_q(model_name):
    if platform.system() == "Windows":
        print("âš  Windowsì—ì„œëŠ” 4bit ë¶ˆê°€ â†’ FP16ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map='auto'
        )
    else:
        print("ğŸ”µ Linux/RunPod í™˜ê²½: 4bit ì—†ì´ bf16ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,   # ì•ˆ ë˜ë©´ torch.float16 ë¡œ ë°”ê¿”ë„ ë¨
            device_map="auto",
        )

    text_gen_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        return_full_text=False, 
        max_new_tokens = 2048, 
        temperature=0.2,
        top_p=0.9
    )

    llm = HuggingFacePipeline(pipeline=text_gen_pipe)
    chat_llm = ChatHuggingFace(llm=llm)
    return chat_llm

# agent ìƒì„± 
def build_agent(llm, vector_store, default_domain="IT"):

    system_prompt = ChatPromptTemplate.from_template("""
        You are an AI meeting-analysis agent specialized in IT projects and software development.
        You will receive user requests and (often) a meeting transcript about IT topics
        (e.g., architecture, infra, APIs, CI/CD, data, AI/ML, product decisions).
        
        You must answer as accurately as possible using the available tools.
        
        You have access to the following tools:
        
        {tools}
        
        Your goals when handling a meeting-related request are:
        1) Understand the meeting context (purpose, participants, decisions, open issues).
        2) When necessary, clarify or look up IT/technical terms or concepts using tools.
        3) When the user asks, summarize the meeting, extract decisions, action items, risks, or follow-up tasks.
        4) Ground your answers in the meeting transcript and retrieved IT-domain documents; avoid hallucinating
           requirements or decisions that are not supported by the content.
        
        Use the following ReAct-style format:
        
        Question: the input question or request you must answer
        Thought: you should always think about what to do next
        Action: the action to take, should be one of [{tool_names}]
        Action Input: the input to the action
        Observation: the result of the action
        ... (this Thought/Action/Action Input/Observation can repeat N times)
        Thought: I now know the final answer
        Final Answer: the final answer to the original input question in Korean
        
        Important rules:
        - If the user request is general chit-chat, a simple greeting, or a very simple question,
          you MAY skip Action/Action Input/Observation and respond directly with Final Answer.
        - If you need additional IT knowledge, definitions, or related internal documents,
          choose the most appropriate tool from [{tool_names}] and use it.
        - Use the meeting transcript and retrieved documents as the primary source of truth.
        - When you summarize or extract tasks/decisions, be faithful to the transcript.
        - Final Answer MUST be written in Korean, unless the user clearly asks for another language.
        
        Begin!
        
        Question: {query}
        Thought:{agent_scratchpad}
        """)

    tools = []
    tools.append(
        Tool(
            name="lookup_definition",
            func=lambda q: lookup_definition(q, vector_store, default_filter),
            description="íšŒì˜ë¡ ì „ë¬¸ì— ëª¨í˜¸í•œ ë‹¨ì–´ê°€ í¬í•¨ëì„ ë•Œ, ë¬¸ì„œì˜ ë‹¨ì–´ ì •ì˜ë¥¼ ì°¸ê³ í•´ì„œ ìš”ì•½ ë° íƒœìŠ¤í¬ ì¶”ì¶œ",
            return_direct=False
        )
    )

    # agent = create_tool_calling_agent(model, tools, prompt)
    agent = create_react_agent(llm, tools, system_prompt)

    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=False,
        max_iterations=30,
        max_execution_time=60,
        handle_parsing_errors=True,
    )

    return agent_executor

# ===== ë„ë©”ì¸ í•„í„° =====
def make_filter(filter: dict):
    if any(filter.values()):
        main_filter = filter.copy()
    else:
        main_filter = None
    return main_filter

# ì •ì˜ ë°˜í™˜
@tool("lookup_definition")
def lookup_definition(terms: str) -> str:
    """IT ìš©ì–´ë‚˜ íšŒì˜ì—ì„œ ë“±ì¥í•œ ëª¨í˜¸í•œ ë‹¨ì–´ë¥¼ ë²¡í„°DBì—ì„œ ê²€ìƒ‰í•˜ì—¬ ì •ì˜ë¥¼ ë°˜í™˜í•œë‹¤."""
    retriever = vector_store.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={
            "score_threshold": 0.8,
            "filter": {"domain": default_domain},
        },
    )
    term_list = [t.strip() for t in re.split(r"[,;\n]+", terms) if t.strip()]
    lines = []
    for term in term_list:
        docs = retriever.invoke(term)
        if not docs:
            return f"'{term}'ì— ëŒ€í•œ ì •ì˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        defs = []
        for d in docs[:3]:
            ans = d.metadata.get("answer") or d.page_content
            defs.append(ans.strip())
        lines.append(f"{term}:\n" + "\n\n".join(defs))
    # print('ğŸ‹ ëª¨ë¥´ëŠ” ë‹¨ì–´:', term_list)
    return "\n\n---\n\n".join(lines), term_list


def make_chain(model):
    instruction = """
    ë‹¹ì‹ ì€ íšŒì˜ë¡ ì „ë¬¸ì„ ë¶„ì„í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤.
    ì•„ë˜ ì§€ì¹¨ì„ ë”°ë¼ JSON í•˜ë‚˜ë§Œ ìƒì„±í•˜ì„¸ìš”.

    1) íšŒì˜ ìš”ì•½(summary)
       - 3~6ë¬¸ì¥ ì •ë„ì˜ í•œêµ­ì–´ ë¬¸ë‹¨ ë˜ëŠ” 5~10ê°œ ë¶ˆë¦¿ìœ¼ë¡œ,
         ëˆ„ê°€ ì–´ë–¤ ê²°ì •ì„ í–ˆê³ , ì–´ë–¤ ê¸°ì¤€/ì§€í‘œ/ì œì•½ì´ ë…¼ì˜ë˜ì—ˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
       - ê°€ëŠ¥í•˜ë©´ PM/PO/QA/FE/BE/AI ë“± ì—­í• ë³„ë¡œ í•µì‹¬ ë°œì–¸ì„ ì •ë¦¬

    2) íƒœìŠ¤í¬(tasks)
       - íšŒì˜ì—ì„œ ì‹¤ì œë¡œ "í•´ì•¼ í•  ì¼"ë¡œ ë“¤ë¦¬ëŠ” ë‚´ìš©ì„ ìµœëŒ€í•œ ì˜ê²Œ ìª¼ê°œì„œ ì¶”ì¶œ
       - ê° íƒœìŠ¤í¬ëŠ” ì•„ë˜ í•„ë“œë¥¼ ê°€ì§„ ê°ì²´:
         - owner: ë‹´ë‹¹ì ì´ë¦„ ë˜ëŠ” ì—­í•  (ì˜ˆ: "ë°•ì§€ì€(PM)", "ê¹€í˜„ìš°(PO)")
         - task: êµ¬ì²´ì ì¸ í–‰ë™ ë¬¸ì¥ (ì˜ˆ: "MVP Must/Should/Won't ë¦¬ìŠ¤íŠ¸ ë¬¸ì„œí™”")
         - due: ê¸°í•œ. íšŒì˜ì—ì„œ ëª…ì‹œëìœ¼ë©´ êµ¬ì²´ ë‚ ì§œ, ì—†ìœ¼ë©´ "TBD" ë˜ëŠ” "" ì‚¬ìš©

    3) definitions í™œìš©
       - {rag_result_text} ì—ëŠ” íšŒì˜ì—ì„œ ì‚¬ìš©ëœ ìš©ì–´ì— ëŒ€í•œ ì •ì˜ JSONì´ ë“¤ì–´ìˆë‹¤ê³  ê°€ì •
       - í•´ë‹¹ ìš©ì–´ë“¤ì´ ë“±ì¥í•˜ë©´, ê·¸ ë§¥ë½ì„ ì´í•´í•˜ëŠ” ë° ì°¸ê³ ë§Œ í•˜ê³ ,
         summary / tasks ì•ˆì— ë¶ˆí•„ìš”í•˜ê²Œ ê·¸ëŒ€ë¡œ ë³µë¶™í•˜ì§€ëŠ” ë§ˆì„¸ìš”.

    ì¶œë ¥ í˜•ì‹:
    - ë°˜ë“œì‹œ ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆ í•œ ê°œë§Œ ë°˜í™˜í•˜ì„¸ìš”.
    - keysë¥¼ ì¤‘ë³µ ì •ì˜í•˜ì§€ ë§ˆì„¸ìš”. (ì˜ˆ: "tasks"ë¥¼ ë‘ ë²ˆ ì“°ì§€ ë§ ê²ƒ)

    {{
      "summary": "<ìì„¸í•œ í•œêµ­ì–´ ìš”ì•½>",
      "tasks": [
        {{
          "who": "ì´ë¦„ ë˜ëŠ” ì—­í• ",
          "what": "í•´ì•¼ í•  ì¼",
          "when": "YYYY-MM-DD ë˜ëŠ” 'TBD' í˜¹ì€ ë¹ˆ ë¬¸ìì—´"
        }},
        ...
      ]
    }}
    """

    parser = StrOutputParser()
    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(instruction),
        HumanMessagePromptTemplate.from_template(
            "ì‚¬ìš©ì íšŒì˜ë¡: {text}\n\n"
            "ì°¸ê³  ë¬¸ì„œ:\n{rag_result_text}\n\n"
            "ìœ„ì˜ ì§€ì¹¨ì„ ì¤€ìˆ˜í•˜ì—¬ ì˜¤ì§ ì‚¬ìš©ìê°€ ì…ë ¥í•˜ëŠ” íšŒì˜ë¡ì— ëŒ€í•œ ë‹µë³€ë§Œ ìƒì„±í•´ì•¼ í•´."
        )
    ])
    chain = prompt | model | parser
    return chain

# ===== ë©”ì¸ =====
if __name__ == "__main__":
    load_dotenv()
    db_path = './faiss_db/rag_it_tta'
    HF_TOKEN = os.getenv('HF_TOKEN')
    if HF_TOKEN:
        login(token=HF_TOKEN, add_to_git_credential=False)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("Device set to:", device)

    vector_store, embedding_model = load_faiss_db(db_path)

    model_name = "Qwen/Qwen2.5-1.5B-Instruct"
    model = load_model_q(model_name)

    ft_model_name = "Qwen/Qwen2.5-1.5B-Instruct"
    ft_model = load_model_q(ft_model_name) 

    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = build_agent(model, vector_store, default_domain='IT')

    # íŒŒíŠœ ëª¨ë¸
    chain = make_chain(ft_model)
    print("íšŒì˜ë¡ ì „ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”! ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥\n")

    while True:
        query = input("ì „ë¬¸: ")
        if query.lower() in ["exit", "quit"]:
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ğŸ‘‰ ì—ì´ì „íŠ¸ì—ê²Œ ê·¸ëƒ¥ í†µìœ¼ë¡œ ë˜ì§„ë‹¤?????????
        # result = agent.invoke({"input": query})
        # result = agent.invoke({"messages": [{"role": "user", "input": query}]})

        print("\n--- ğŸ” ì—ì´ì „íŠ¸: ë‹¨ì–´ ì •ì˜ ì¶”ì¶œì¤‘ ---")
        agent_result = agent.invoke({"query": query})
        rag_result_text = agent_result["output"]
        print(" ğŸ” ì—ì´ì „íŠ¸ definitions:", rag_result_text)

        print("\n--- ğŸ¤– íŒŒíŠœ ëª¨ë¸: ìš”ì•½ ìƒì„±ì¤‘ ---")
        result = chain.invoke({
            "text": query,
            "rag_result_text": rag_result_text
        })

        # AgentExecutorëŠ” ë³´í†µ {"output": "...", ...} í˜•íƒœ ë°˜í™˜
        print("\nëª¨ë¸ ì‘ë‹µ(JSON):\n", result)