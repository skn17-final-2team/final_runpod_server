import os, time, torch, platform, re, json 
from dotenv import load_dotenv
from huggingface_hub import login
from pathlib import Path

from peft import PeftModel
from langchain.vectorstores import FAISS
from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace

from typing import Any, List, Optional
from langchain_core.language_models.llms import LLM
from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.prompt_values import PromptValue
from langchain_core.messages import BaseMessage


os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"

PROMPT_DIR = Path(__file__).parent / "prompts"
SYSTEM_PROMPT = (PROMPT_DIR / "system.txt").read_text(encoding="utf-8").strip()
PROMPTS = {
    "summarizer": (PROMPT_DIR / "summarizer.txt").read_text(encoding="utf-8").strip(),
    "task_extractor": (PROMPT_DIR / "extract_tasks.txt").read_text(encoding="utf-8").strip(),
}

base_model_name = "Qwen/Qwen2.5-1.5B-Instruct"
ft_model_name = "CHOROROK/Qwen2.5_1.5B_trained_model_v3"


# ===== ì´ìŠ¤ì¼€ì´íŠ¸ ====
def escape_curly(text: str) -> str:
    return text.replace("{", "{{").replace("}", "}}")


# ===== ë²¡í„° DB ë¡œë“œ =====
def load_faiss_db(db_path: str):
    embedding_model = HuggingFaceEmbeddings(model_name="dragonkue/snowflake-arctic-embed-l-v2.0-ko")
    vector_store = FAISS.load_local(db_path, embedding_model, allow_dangerous_deserialization=True)
    print("ğŸ”µ FAISS DB ë¡œë“œ ì™„ë£Œ!\n")
    return vector_store, embedding_model


# ===== í˜•ì‹ì§€ì • =====
class HFTextGenLLM(LLM):
    """HF text-generation pipelineì„ ê°ì‹¸ëŠ”, ë¹„-ìŠ¤íŠ¸ë¦¬ë° LLM ë˜í¼."""

    # ğŸ”¹ BaseModel(LLM)ì— ë“±ë¡ë  í•„ë“œ
    pipe: Any

    # ğŸ”¹ HF pipeline ê°™ì€ ì„ì˜ íƒ€ì…ì„ í•„ë“œë¡œ í—ˆìš©í•˜ë ¤ë©´ ê¼­ í•„ìš”
    model_config = {
        "arbitrary_types_allowed": True,
    }

    @property
    def _llm_type(self) -> str:
        return "hf_text_generation_pipeline"

    def _normalize_prompt(self, prompt) -> str:
        """LangChain ìª½ì—ì„œ ë„˜ì–´ì˜¤ëŠ” ë‹¤ì–‘í•œ íƒ€ì…ì„ í•­ìƒ strë¡œ ì •ê·œí™”."""
        # 1) PromptValue (PromptTemplate | ChatPromptTemplate ê²°ê³¼)
        if isinstance(prompt, PromptValue):
            return prompt.to_string()

        # 2) ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        if isinstance(prompt, list) and prompt:
            if isinstance(prompt[0], BaseMessage):
                return "\n".join(m.content for m in prompt)

        # 3) ì´ë¯¸ ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ
        if isinstance(prompt, str):
            return prompt

        # 4) ë‚˜ë¨¸ì§€ëŠ” ê·¸ëƒ¥ ë¬¸ìì—´ ìºìŠ¤íŒ…
        return str(prompt)

    def _call(
        self,
        prompt,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs,
    ) -> str:
        """LangChainì—ì„œ í˜¸ì¶œí•˜ëŠ” ë©”ì¸ ì—”íŠ¸ë¦¬. ë‚´ë¶€ì ìœ¼ë¡œ HF pipeline í•œ ë²ˆ ì‹¤í–‰."""
        text = self._normalize_prompt(prompt)

        # HF text-generation pipeline ì‹¤í–‰
        outputs = self.pipe(text)

        # transformers pipeline("text-generation") ê¸°ë³¸ ë°˜í™˜ í˜•ì‹: [{"generated_text": "..."}]
        if not outputs:
            return ""

        first = outputs[0]
        generated = first.get("generated_text") or first.get("text") or ""

        # stop í† í°ì´ ìˆìœ¼ë©´ ê±°ê¸°ì„œ ì˜ë¼ì£¼ê¸°
        if stop:
            for s in stop:
                if s in generated:
                    generated = generated.split(s)[0]
                    break

        return generated


# ===== ëª¨ë¸ ë¡œë“œ =====
def load_model_q(model_name: str , adapter_name: str | None = ft_model_name):
    if platform.system() == "Windows":
        print("âš  Windowsì—ì„œëŠ” 4bit ë¶ˆê°€ â†’ FP16ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.")
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map='auto'
        )
    else:
        print("ğŸ”µ Linux/RunPod í™˜ê²½: 4bit ì—†ì´ bf16ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.")
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.bfloat16,   # ì•ˆ ë˜ë©´ torch.float16 ë¡œ ë°”ê¿”ë„ ë¨
            device_map="auto",
        )

    if adapter_name:
        print(f"ğŸ”µ LoRA/PEFT ì–´ëŒ‘í„° ë¡œë“œ: {adapter_name}")
        model = PeftModel.from_pretrained(base_model, adapter_name)
    else:
        model = base_model

    text_gen_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        return_full_text=False, 
        max_new_tokens = 2048, 
        temperature=0.2,
        top_p=0.9
    )

    llm = HFTextGenLLM(pipe = text_gen_pipe)
    return llm


# ===== ë„ë©”ì¸ í•„í„° =====
def make_filter(filter: dict):
    if any(filter.values()):
        main_filter = filter.copy()
    else:
        main_filter = None
    return main_filter