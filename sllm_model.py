import os
import json
import platform
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Any, Tuple

from langchain.tools import tool
from langchain.agents import create_react_agent, AgentExecutor, Tool
from langchain.prompts import PromptTemplate

from main_model import load_model_q, load_faiss_db, escape_curly

# ===== ê¸°ë³¸ì„¤ì • =====
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"

db_path = './faiss_db_merged'
vector_store, embedding_model = load_faiss_db(db_path)

PROMPT_DIR = Path(__file__).parent / "prompts"
SYSTEM_PROMPT = (PROMPT_DIR / "system.txt").read_text(encoding="utf-8").strip()
PROMPTS = {
    "summarizer": (PROMPT_DIR / "summarizer.txt").read_text(encoding="utf-8").strip(),
    "task_extractor": (PROMPT_DIR / "extract_tasks.txt").read_text(encoding="utf-8").strip(),
}

summarizer_prompt = PROMPTS["summarizer"]
task_prompt = PROMPTS["task_extractor"]


# ===== ì²­í¬ ë¶„í•  (1500ì ê¸°ì¤€ ì• ë’¤ ë¬¸ì¥) =====
def chunk_transcript(transcript: str, max_tokens: int = 1500) -> List[str]:
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° ê¸°ì¤€)
    sentences = []
    current = ""
    for char in transcript:
        current += char
        if char in ['.', '!', '?', '\n'] or (char == 'ë‹¤' and len(current) > 20):
            if current.strip():
                sentences.append(current.strip())
            current = ""
    if current.strip():
        sentences.append(current.strip())

    # ì²­í¬ ìƒì„±
    chunks = []
    current_chunk = ""
    current_length = 0

    for sentence in sentences:
        sentence_length = len(sentence)

        if current_length + sentence_length > max_tokens and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = sentence
            current_length = sentence_length
        else:
            current_chunk += " " + sentence
            current_length += sentence_length

    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    return chunks


# ===== ì²­í¬ ë³„ ì²˜ë¦¬ ë° ì „ì²´ ìš”ì•½/íƒœìŠ¤í¬ ì¶”ì¶œ =====
def process_transcript_with_chunks(agent, transcript: str, max_chunk_tokens: int = 1500) -> dict:
    
    print(f"\n{'='*60}")
    print(f"ì „ë¬¸ ê¸¸ì´: {len(transcript)} ê¸€ì")
    print(f"{'='*60}\n")

    # ì „ë¬¸ ê¸¸ ê²½ìš° - ì²­í¬ 
    if len(transcript) > max_chunk_tokens:
        chunks = chunk_transcript(transcript, max_chunk_tokens)
        print(f"ì „ë¬¸ì„ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.\n")

        chunk_results = []
        for i, chunk in enumerate(chunks, 1):
            print(f"\n[ì²­í¬ {i}/{len(chunks)}] ì²˜ë¦¬ ì¤‘... (ê¸¸ì´: {len(chunk)} ê¸€ì)")
            try:
                # ê° ì²­í¬ agent ì²˜ë¦¬ (ìš©ì–´ ê²€ìƒ‰)
                result = agent.invoke({"input": f"ë‹¤ìŒì€ íšŒì˜ë¡ì˜ ì¼ë¶€ì…ë‹ˆë‹¤. ì´í•´í•˜ê¸° ì–´ë ¤ìš´ ì „ë¬¸ ìš©ì–´ê°€ ìˆìœ¼ë©´ ê²€ìƒ‰í•´ì£¼ì„¸ìš”:\n\n{chunk}"})
                chunk_results.append({"chunk_index": i, "chunk_length": len(chunk), "result": result.get("output", "")})
                print(f"[ì²­í¬ {i}] ì²˜ë¦¬ ì™„ë£Œ")
            except Exception as e:
                print(f"[ì²­í¬ {i}] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                chunk_results.append({
                    "chunk_index": i,
                    "chunk_length": len(chunk),
                    "error": str(e)
                })
    else:
      # ì „ë¬¸ ê¸¸ì´ ì ì ˆ ì‹œ - í’€ë¡œ ì§„í–‰
      print("ì „ë¬¸ ê¸¸ì´ê°€ ì ì ˆí•©ë‹ˆë‹¤. ì²­í¬ ë¶„í•  ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.\n")
      chunk_results = []

    # ì „ì²´ ì „ë¬¸ ê¸°ë°˜ìœ¼ë¡œ ì„¸ë¶€ ì•ˆê±´, ì•ˆê±´ë³„ ìš”ì•½ ì¶”ì¶œ
    print(f"\n{'='*60}")
    print("ì „ì²´ ì „ë¬¸ ê¸°ë°˜ ì•ˆê±´/ìš”ì•½ ì¶”ì¶œ ì¤‘...")
    print(f"{'='*60}\n")

    try:
        summary_result = agent.invoke({"input": summarizer_prompt})
        full_summary = summary_result.get("output", "")
        print("âœ… ì•ˆê±´/ìš”ì•½ ì¶”ì¶œ ì™„ë£Œ\n")
    except Exception as e:
        print(f"âŒ ì•ˆê±´/ìš”ì•½ ì¶”ì¶œ ì‹¤íŒ¨: {e}\n")
        full_summary = {"error": str(e)}

    print(f"\n{'='*60}")
    print("ì „ì²´ ì „ë¬¸ ê¸°ë°˜ íƒœìŠ¤í¬ ì¶”ì¶œ ì¤‘...")
    print(f"{'='*60}\n")

    try:
        task_result = agent.invoke({"input": task_prompt})
        full_tasks = task_result.get("output", "")
        print("âœ… íƒœìŠ¤í¬ ì¶”ì¶œ ì™„ë£Œ\n")
    except Exception as e:
        print(f"âŒ íƒœìŠ¤í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}\n")
        full_tasks = {"error": str(e)}

    return {"chunk_results": chunk_results, "full_summary": full_summary, "full_tasks": full_tasks}


# ===== agent!!! =====
def build_agent(model, vector_store, domain) :

    safe_summarizer = escape_curly(PROMPTS["summarizer"])
    safe_task_prompt = escape_curly(PROMPTS["task_extractor"])

    search_kwargs = {"k": 20}
    if domain:
        search_kwargs["filter"] = {"domain": domain}

    retriever = vector_store.as_retriever(search_kwargs=search_kwargs)

    template = '''
        You are an AI meeting-analysis agent specialized in {domain} projects.
        You will receive a meeting transcript about {domain} topics.
        (e.g., Analysis of the latest market trends and competitor movements, Project Kick-off Meeting).

        You must answer as accurately as possible using the available tools.
        You have access to the following tools:
        {tools}

        Your primary goals when handling a meeting-related request are:
        1) Understand the meeting context:
           - ëª©ì (purpose), ì°¸ì—¬ì(participants), ê²°ì •ì‚¬í•­(decisions), ë¯¸í•´ê²° ì´ìŠˆ(open issues)ë¥¼ íŒŒì•…í•œë‹¤.
        2) When necessary, clarify or look up IT/technical/domain terms or concepts using tools.
        3) From the meeting transcript, you must be able to:
           - extract detailed agenda list
           - summarize the meeting
           - extract follow-up tasks
        4) Ground your answers in the meeting transcript and retrieved domain documents; 
           NEVER hallucinate requirements or decisions that are not supported by the content.

          - Every field MUST be filled ONLY with words/phrases that appear in the original transcript.
          - If a certain 5W3H item is not explicitly stated in the transcript, set that field to null.
          - Do NOT invent or infer new facts that are not present in the transcript.

        Use the following ReAct-style format:

        transcript: the input transcript for the meeting
        Thought: you should always think about what to do next
        Action: the action to take, should be one of [{tool_names}]
        Action Input: the input to the action
        Observation: the result of the action
        ... (this Thought/Action/Action Input/Observation can repeat up to 15 times)
        Thought: I now know the final answer
        Final Answer: the final answer to the original input question in Korean
        (When you need JSON outputs, internally follow the Summary JSON Prompt
         or Tasks JSON Prompt described above.)

        Hard constraints (format rules â€“ MUST NOT be violated):
        - Immediately after any line that starts with "Thought:", the very next line MUST be one of the following:
          1) "Action: ..."
          2) "Final Answer: ..."
        - Do NOT write bullet points, long explanations, or any additional sentences between "Thought:" and
          the next "Action:" or "Final Answer:". The line immediately following "Thought:" must be exactly
          one of those two formats.
        - When you use a tool, you MUST follow this format exactly:
          Thought: ...
          Action: tool_name
          Action Input: "the input to pass to the tool"
          Observation: the result returned by the tool
        - When you no longer need to use any tools and you want to finish the answer, you MUST follow this format:
          Thought: I can now provide the final answer.
          Final Answer: (write the final answer in Korean)

        Important rules:
        - If the user request is general chit-chat, a simple greeting, or a very simple question, you MAY skip Action/Action Input/Observation and respond directly with Final Answer.
        - If you need additional domain knowledge or definitions, choose the most appropriate tool from [{tool_names}] and use it.
        - Use the meeting transcript and retrieved documents as the primary source of truth.
        - When you summarize or extract issues/decisions/tasks, be faithful to the transcript.
        - Final Answer MUST be written in Korean, unless the user clearly asks for another language.

        Begin!

        transcript:{input}
        Thought:{agent_scratchpad}
        domain:{domain}
        '''

    prompt = PromptTemplate.from_template(template).partial(domain=domain)

    # retrieval tool
    def retrieval_func(term_list: str) -> dict:
        """
        FAISS ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì „ë¬¸ ì¤‘ ëª¨ë¥´ëŠ” ë‹¨ì–´ë¥¼ ê²€ìƒ‰í•´ ë‹¨ì–´ ì •ì˜ë¥¼ ë°˜í™˜í•˜ëŠ” íˆ´.
        Args: term_list: ê²€ìƒ‰í•  ìš©ì–´ë“¤ (ë‹¨ì¼ ë¬¸ìì—´ ë˜ëŠ” JSON í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´)
             ì˜ˆ: "API" ë˜ëŠ” '["API", "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤"]'
        Returns: ê²€ìƒ‰ëœ ìš©ì–´ë“¤ì˜ ì •ì˜ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        """
        print('='*60)
        print('ğŸ” retrieval_func í˜¸ì¶œë¨')
        print('ì…ë ¥ê°’:', term_list)
        print('ì…ë ¥ íƒ€ì…:', type(term_list))
        print("í˜„ì¬ domain í•„í„° = ", domain)
        print('='*60)

        # ì…ë ¥ ì •ê·œí™”
        try:
            if isinstance(term_list, str):
                term_list = term_list.strip()
                if term_list.startswith('[') and term_list.endswith(']'):
                    try:
                        term_list_local = json.loads(term_list)
                        if not isinstance(term_list_local, list):
                            term_list_local = [str(term_list_local)]
                    except json.JSONDecodeError as je:
                        print(f'âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {je}, ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì‚¬ìš©')
                        # ëŒ€ê´„í˜¸ ì œê±°í•˜ê³  ì‰¼í‘œë¡œ split
                        term_list_local = [t.strip().strip('"\'') for t in term_list.strip('[]').split(',')]
                elif ',' in term_list:
                    term_list_local = [t.strip().strip('"\'') for t in term_list.split(',')]
                else:
                    term_list_local = [term_list]

            elif isinstance(term_list, list):
                term_list_local = term_list

            elif isinstance(term_list, tuple):
                term_list_local = list(term_list)

            else:
                print(f'âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…: {type(term_list)}, ë¬¸ìì—´ë¡œ ë³€í™˜')
                term_list_local = [str(term_list)]

            print('íŒŒì‹±ëœ term_list_local:', term_list_local)
            term_list_local = [str(term).strip().strip('"\'') for term in term_list_local if term]
            term_list_local = [t for t in term_list_local if t]
            print('ìµœì¢… ì •ë¦¬ëœ term_list_local:', term_list_local)

        except Exception as parse_error:
            print(f'âŒ ì…ë ¥ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {type(parse_error).__name__}: {parse_error}')
            print(f'ì›ë³¸ ì…ë ¥ì„ ë‹¨ì¼ í•­ëª©ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤: {repr(term_list)}')
            term_list_local = [str(term_list).strip()]

        if not term_list_local:
            print('ê²€ìƒ‰í•  ìš©ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.')
            return {"output": json.dumps({"definitions": {}}, ensure_ascii=False)}

        # ê°œë³„ ê²€ìƒ‰ ì²˜ë¦¬
        all_docs_list = []
        k = search_kwargs.get("k", 20)
        filter_dict = search_kwargs.get("filter", None)

        for idx, term in enumerate(term_list_local):
            try:
                # termì„ ëª…ì‹œì ìœ¼ë¡œ ë¬¸ìì—´ë¡œ ë³€í™˜
                term_str = str(term).strip()
                print(f'\n[{idx+1}/{len(term_list_local)}] ê²€ìƒ‰ ì‹œì‘')
                print(f'  ì›ë³¸ term: {repr(term)} (íƒ€ì…: {type(term).__name__})')
                print(f'  ë³€í™˜ term_str: {repr(term_str)} (íƒ€ì…: {type(term_str).__name__})')

                if not term_str:
                    print(f'âš ï¸ ë¹ˆ ê²€ìƒ‰ì–´ ê±´ë„ˆëœ€')
                    all_docs_list.append([])
                    continue

                print(f'  ğŸ” FAISS ê²€ìƒ‰ ì‹¤í–‰: "{term_str}"')
                print(f'  ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: k={k}, filter={filter_dict}')

                # query íƒ€ì… ì¬í™•ì¸
                if not isinstance(term_str, str):
                    raise TypeError(f"query must be str, got {type(term_str).__name__}")

                if len(term_str) == 0:
                    print(f'âš ï¸ ë¹ˆ ë¬¸ìì—´, ê±´ë„ˆëœ€')
                    all_docs_list.append([])
                    continue

                # FAISS ê²€ìƒ‰ ì‹¤í–‰
                docs = vector_store.similarity_search(
                    term_str,  # í‚¤ì›Œë“œ ì¸ì ëŒ€ì‹  ìœ„ì¹˜ ì¸ì ì‚¬ìš©
                    k=k,
                    filter=filter_dict
                )
                all_docs_list.append(docs)
                print(f'  âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ ë°œê²¬')
            
            except Exception as e:
                print(f'  âŒ ê²€ìƒ‰ ì‹¤íŒ¨!')
                print(f'     ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}')
                print(f'     ì˜¤ë¥˜ ë©”ì‹œì§€: {e}')
                print(f'     ë¬¸ì œëœ term: {repr(term)}')
                
                import traceback
                traceback.print_exc()
                all_docs_list.append([])

        definitions = {}
        for term, docs in zip(term_list_local, all_docs_list):
            if not docs:
                print(f'@#$@#$@${term}ì— ëŒ€í•œ ìš©ì–´ ëª»ì°¾ìŒ@#$#@$@#')
                continue

            defs = []
            for d in docs[:2]:
                ans = d.metadata.get("answer") or d.page_content
                defs.append(ans.strip())

            definitions[term] = "\n\n".join(defs)

        print('ì—¬ê¸°ë‹¤ ì—¬ê¸°!!! ë‹¨ì–´ ì •ì˜ : ', definitions)
        retrieval_result = {"output": json.dumps({"definitions": definitions}, ensure_ascii=False)}

        return retrieval_result

    # Tool ê°ì²´ ì§ì ‘ ìƒì„±
    from pydantic import BaseModel, Field

    class RetrievalInput(BaseModel):
        term_list: str = Field(description="ê²€ìƒ‰í•  ìš©ì–´ë“¤ (ë‹¨ì¼ ë¬¸ìì—´ ë˜ëŠ” JSON ë°°ì—´ ë¬¸ìì—´)")

    retrieval_tool = Tool(
        name="retrieval",
        func=retrieval_func,
        description="FAISS ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì „ë¬¸ ì¤‘ ëª¨ë¥´ëŠ” ë‹¨ì–´ë¥¼ ê²€ìƒ‰í•´ ë‹¨ì–´ ì •ì˜ë¥¼ ë°˜í™˜í•˜ëŠ” íˆ´. ì…ë ¥ì€ JSON ë°°ì—´ í˜•ì‹ì˜ ë¬¸ìì—´ ë˜ëŠ” ë‹¨ì¼ ìš©ì–´ ë¬¸ìì—´.",
        args_schema=RetrievalInput
    )

    tools = [retrieval_tool]

    agent = create_react_agent(model, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False, max_iterations=20, max_execution_time=400, handle_parsing_errors=True)

    return agent_executor


