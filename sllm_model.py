import os
import json
import platform
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Any, Tuple

from langchain.tools import tool
from langchain.agents import create_react_agent, AgentExecutor, Tool
from langchain.prompts import PromptTemplate
from datetime import datetime

from main_model import load_model_q, load_faiss_db, escape_curly, preprocess_transcript

# ===== ê¸°ë³¸ì„¤ì • =====
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"

db_path = "./faiss_db_merged"
vector_store, embedding_model = load_faiss_db(db_path)

PROMPT_DIR = Path(__file__).parent / "prompts"
SYSTEM_PROMPT = (PROMPT_DIR / "system.txt").read_text(encoding="utf-8").strip()
PROMPTS = {
    "summarizer": (PROMPT_DIR / "summarizer.txt").read_text(encoding="utf-8").strip(),
    "task_extractor": (PROMPT_DIR / "extract_tasks.txt").read_text(encoding="utf-8").strip(),
}

summarizer_prompt = PROMPTS["summarizer"]
task_prompt = PROMPTS["task_extractor"]


# ===== ì²­í¬ ë¶„í•  (1500ì ê¸°ì¤€ ì• ë’¤ ë¬¸ì¥) =====
def chunk_transcript(transcript: str, max_tokens: int = 1500) -> List[str]:
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° ê¸°ì¤€)
    sentences = []
    current = ""
    for char in transcript:
        current += char
        if char in ['.', '!', '?', '\n'] or (char == 'ë‹¤' and len(current) > 20):
            if current.strip():
                sentences.append(current.strip())
            current = ""
    if current.strip():
        sentences.append(current.strip())

    # ì²­í¬ ìƒì„±
    chunks = []
    current_chunk = ""
    current_length = 0

    for sentence in sentences:
        sentence_length = len(sentence)

        if current_length + sentence_length > max_tokens and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = sentence
            current_length = sentence_length
        else:
            current_chunk += " " + sentence
            current_length += sentence_length

    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    return chunks


# ===== ì²­í¬ ë³„ ì²˜ë¦¬ ë° ì „ì²´ ìš”ì•½/íƒœìŠ¤í¬ ì¶”ì¶œ =====
def process_transcript_with_chunks(transcript: str, domain) -> dict:

    user_domain = domain
    if not user_domain :
        domain_filter = None
    else:
        domain_filter = user_domain
    
    # ì „ë¬¸ ë¬¸ì¥ã„¹ë¡œ 
    transcript = preprocess_transcript(transcript)    

    # ì—ì´ì „íŠ¸ ë¹Œë“œ
    agent = build_agent(model=load_model_q(), vector_store=vector_store, domain=domain_filter)
    # max_chunk_tokens = 1500

    print(f"\n{'='*60}")
    print(f"ì „ë¬¸ ê¸¸ì´: {len(transcript)} ê¸€ì")
    print(f"{'='*60}\n")

    # # ì „ë¬¸ ê¸¸ ê²½ìš° - ì²­í¬ 
    # if len(transcript) > max_chunk_tokens:
    #     chunks = chunk_transcript(transcript, max_chunk_tokens)
    #     print(f"ì „ë¬¸ì„ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.\n")

    #     chunk_results = []
    #     for i, chunk in enumerate(chunks, 1):
    #         print(f"\n[ì²­í¬ {i}/{len(chunks)}] ì²˜ë¦¬ ì¤‘... (ê¸¸ì´: {len(chunk)} ê¸€ì)")
    #         try:
    #             # ê° ì²­í¬ agent ì²˜ë¦¬ (ìš©ì–´ ê²€ìƒ‰)
    #             result = agent.invoke({"input": f"ë‹¤ìŒì€ íšŒì˜ë¡ì˜ ì¼ë¶€ì…ë‹ˆë‹¤. ì´í•´í•˜ê¸° ì–´ë ¤ìš´ ì „ë¬¸ ìš©ì–´ê°€ ìˆìœ¼ë©´ ê²€ìƒ‰í•´ì£¼ì„¸ìš”:\n\n{chunk}"})
    #             chunk_results.append({"chunk_index": i, "chunk_length": len(chunk), "result": result.get("output", "")})
    #             print(f"[ì²­í¬ {i}] ì²˜ë¦¬ ì™„ë£Œ")
    #         except Exception as e:
    #             print(f"[ì²­í¬ {i}] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    #             chunk_results.append({
    #                 "chunk_index": i,
    #                 "chunk_length": len(chunk),
    #                 "error": str(e)
    #             })
    # else:
    #   # ì „ë¬¸ ê¸¸ì´ ì ì ˆ ì‹œ - í’€ë¡œ ì§„í–‰
    #   print("ì „ë¬¸ ê¸¸ì´ê°€ ì ì ˆí•©ë‹ˆë‹¤. ì²­í¬ ë¶„í•  ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.\n")
    #   chunk_results = []

    # # ì „ì²´ ì „ë¬¸ ê¸°ë°˜ìœ¼ë¡œ ì„¸ë¶€ ì•ˆê±´, ì•ˆê±´ë³„ ìš”ì•½ ì¶”ì¶œ
    # print(f"\n{'='*60}")
    # print("ì „ì²´ ì „ë¬¸ ê¸°ë°˜ ì•ˆê±´/ìš”ì•½ ì¶”ì¶œ ì¤‘...")
    # print(f"{'='*60}\n")

    try:
        filled_summary_prompt = f"""ë‹¤ìŒ íšŒì˜ ì „ë¬¸ì„ ë¶„ì„í•˜ì„¸ìš”.
                                    ëª¨ë¥´ëŠ” ì „ë¬¸ ìš©ì–´ê°€ ìˆìœ¼ë©´ retrieval íˆ´ë¡œ ê²€ìƒ‰í•˜ì„¸ìš”.
                                    {summarizer_prompt.format(transcript=transcript)}"""

        summary_result = agent.invoke({"input": filled_summary_prompt})
        full_summary = summary_result.get("output", "")
        print("âœ… ì•ˆê±´/ìš”ì•½ ì¶”ì¶œ ì™„ë£Œ\n")
    except Exception as e:
        print(f"âŒ ì•ˆê±´/ìš”ì•½ ì¶”ì¶œ ì‹¤íŒ¨: {e}\n")
        full_summary = {"error": str(e)}

    print(f"\n{'='*60}")
    print("ì „ì²´ ì „ë¬¸ ê¸°ë°˜ íƒœìŠ¤í¬ ì¶”ì¶œ ì¤‘...")
    print(f"{'='*60}\n")

    try:
        filled_task_prompt = f"""ë‹¤ìŒ íšŒì˜ ì „ë¬¸ì„ ë¶„ì„í•˜ì„¸ìš”.
                                ëª¨ë¥´ëŠ” ì „ë¬¸ ìš©ì–´ê°€ ìˆìœ¼ë©´ retrieval íˆ´ë¡œ ê²€ìƒ‰í•˜ì„¸ìš”.
                                {task_prompt.format(transcript=transcript, current_date=datetime.now())}"""
        task_result = agent.invoke({"input": filled_task_prompt})
        full_tasks = task_result.get("output", "")
        print("âœ… íƒœìŠ¤í¬ ì¶”ì¶œ ì™„ë£Œ\n")
    except Exception as e:
        print(f"âŒ íƒœìŠ¤í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}\n")
        full_tasks = {"error": str(e)}

    # return {"chunk_results": chunk_results, "full_summary": full_summary, "full_tasks": full_tasks}
    return {"full_summary": full_summary, "full_tasks": full_tasks}


# ===== agent!!! =====
def build_agent(model, vector_store, domain) :

    search_kwargs = {"k": 20}
    if domain:
        search_kwargs["filter"] = {"domain": domain}

    retriever = vector_store.as_retriever(search_kwargs=search_kwargs)

    template = '''
        You are an AI meeting-analysis agent specialized in {domain} projects.

        You must answer as accurately as possible using the available tools.
        You have access to the following tools:
        {tools}

        Main duties:
        1. Accurately understand and carry out the given request.
        2. When there are unknown technical or domain-specific terms in the full meeting transcript, use the `retrieval` tool to look them up.
           - In a single call, you MUST search at most 5 terms.
           - Expected input format: ["term1", "term2", "term3"]
        3. Use only information that is explicitly stated in the meeting transcript; do NOT make up or infer information that is not mentioned.
        4. The final answer you provide MUST be written in Korean.

        Use the following ReAct-style format:

        Thought: you should always think about what to do next
        Action: the action to take, should be one of [{tool_names}]
        Action Input: the input to the action
        Observation: the result of the action
        ... (this Thought/Action/Action Input/Observation can repeat up to 3 times)
        Thought: I now know the final answer
        Final Answer: the final answer to the original input question in Korean
        (When you need JSON outputs, internally follow the Summary JSON Prompt
         or Tasks JSON Prompt described above.)

        Begin!

        transcript:{input}
        Thought:{agent_scratchpad}
        domain:{domain}
        '''

    prompt = PromptTemplate.from_template(template).partial(domain=domain)

    # retrieval tool
    def retrieval_func(term_list: str) -> dict:
        """
        FAISS ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì „ë¬¸ ì¤‘ ëª¨ë¥´ëŠ” ë‹¨ì–´ë¥¼ ê²€ìƒ‰í•´ ë‹¨ì–´ ì •ì˜ë¥¼ ë°˜í™˜í•˜ëŠ” íˆ´.
        Args: term_list: ê²€ìƒ‰í•  ìš©ì–´ë“¤ (ë‹¨ì¼ ë¬¸ìì—´ ë˜ëŠ” JSON í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´)
             ì˜ˆ: "API" ë˜ëŠ” '["API", "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤"]'
        Returns: ê²€ìƒ‰ëœ ìš©ì–´ë“¤ì˜ ì •ì˜ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        """
        print('='*60)
        print('ğŸ” retrieval_func í˜¸ì¶œë¨')
        print('ì…ë ¥ê°’:', term_list)
        print('ì…ë ¥ íƒ€ì…:', type(term_list))
        print("í˜„ì¬ domain í•„í„° = ", domain)
        print('='*60)

        # ì…ë ¥ ì •ê·œí™”
        try:
            if isinstance(term_list, str):
                term_list = term_list.strip()
                if term_list.startswith('[') and term_list.endswith(']'):
                    try:
                        term_list_local = json.loads(term_list)
                        if not isinstance(term_list_local, list):
                            term_list_local = [str(term_list_local)]
                    except json.JSONDecodeError as je:
                        print(f'âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {je}, ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì‚¬ìš©')
                        # ëŒ€ê´„í˜¸ ì œê±°í•˜ê³  ì‰¼í‘œë¡œ split
                        term_list_local = [t.strip().strip('"\'') for t in term_list.strip('[]').split(',')]
                elif ',' in term_list:
                    term_list_local = [t.strip().strip('"\'') for t in term_list.split(',')]
                else:
                    term_list_local = [term_list]
            elif isinstance(term_list, list):
                term_list_local = term_list

            elif isinstance(term_list, tuple):
                term_list_local = list(term_list)

            else:
                print(f'âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…: {type(term_list)}, ë¬¸ìì—´ë¡œ ë³€í™˜')
                term_list_local = [str(term_list)]

            print('íŒŒì‹±ëœ term_list_local:', term_list_local)
            term_list_local = [str(term).strip().strip('"\'') for term in term_list_local if term]
            term_list_local = [t for t in term_list_local if t]
            print('ìµœì¢… ì •ë¦¬ëœ term_list_local:', term_list_local)

        except Exception as parse_error:
            print(f'âŒ ì…ë ¥ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {type(parse_error).__name__}: {parse_error}')
            print(f'ì›ë³¸ ì…ë ¥ì„ ë‹¨ì¼ í•­ëª©ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤: {repr(term_list)}')
            term_list_local = [str(term_list).strip()]

        if not term_list_local:
            print('ê²€ìƒ‰í•  ìš©ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.')
            return {"output": json.dumps({"definitions": {}}, ensure_ascii=False)}

        # ê°œë³„ ê²€ìƒ‰ ì²˜ë¦¬
        all_docs_list = []
        k = search_kwargs.get("k", 20)
        filter_dict = search_kwargs.get("filter", None)

        for idx, term in enumerate(term_list_local):
            try:
                # termì„ ëª…ì‹œì ìœ¼ë¡œ ë¬¸ìì—´ë¡œ ë³€í™˜
                term_str = str(term).strip()
                print(f'\n[{idx+1}/{len(term_list_local)}] ê²€ìƒ‰ ì‹œì‘')
                print(f'  ì›ë³¸ term: {repr(term)} (íƒ€ì…: {type(term).__name__})')
                print(f'  ë³€í™˜ term_str: {repr(term_str)} (íƒ€ì…: {type(term_str).__name__})')

                if not term_str:
                    print(f'âš ï¸ ë¹ˆ ê²€ìƒ‰ì–´ ê±´ë„ˆëœ€')
                    all_docs_list.append([])
                    continue

                print(f'  ğŸ” FAISS ê²€ìƒ‰ ì‹¤í–‰: "{term_str}"')
                print(f'  ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: k={k}, filter={filter_dict}')

                # query íƒ€ì… ì¬í™•ì¸
                if not isinstance(term_str, str):
                    raise TypeError(f"query must be str, got {type(term_str).__name__}")

                if len(term_str) == 0:
                    print(f'âš ï¸ ë¹ˆ ë¬¸ìì—´, ê±´ë„ˆëœ€')
                    all_docs_list.append([])
                    continue

                # FAISS ê²€ìƒ‰ ì‹¤í–‰
                docs = vector_store.similarity_search(
                    term_str,  # í‚¤ì›Œë“œ ì¸ì ëŒ€ì‹  ìœ„ì¹˜ ì¸ì ì‚¬ìš©
                    k=k,
                    filter=filter_dict
                )
                all_docs_list.append(docs)
                print(f'  âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ ë°œê²¬')
            
            except Exception as e:
                print(f'  âŒ ê²€ìƒ‰ ì‹¤íŒ¨!')
                print(f'     ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}')
                print(f'     ì˜¤ë¥˜ ë©”ì‹œì§€: {e}')
                print(f'     ë¬¸ì œëœ term: {repr(term)}')
                
                import traceback
                traceback.print_exc()
                all_docs_list.append([])

        definitions = {}
        for term, docs in zip(term_list_local, all_docs_list):
            if not docs:
                print(f'@#$@#$@${term}ì— ëŒ€í•œ ìš©ì–´ ëª»ì°¾ìŒ@#$#@$@#')
                continue

            defs = []
            for d in docs[:2]:
                ans = d.metadata.get("answer") or d.page_content
                defs.append(ans.strip())

            definitions[term] = "\n\n".join(defs)

        print('ì—¬ê¸°ë‹¤ ì—¬ê¸°!!! ë‹¨ì–´ ì •ì˜ : ', definitions)
        retrieval_result = {"output": json.dumps({"definitions": definitions}, ensure_ascii=False)}

        return retrieval_result

    # Tool ê°ì²´ ì§ì ‘ ìƒì„±
    from pydantic import BaseModel, Field

    class RetrievalInput(BaseModel):
        term_list: str = Field(description="ê²€ìƒ‰í•  ìš©ì–´ë“¤ (ë‹¨ì¼ ë¬¸ìì—´ ë˜ëŠ” JSON ë°°ì—´ ë¬¸ìì—´)")

    retrieval_tool = Tool(
        name="retrieval",
        func=retrieval_func,
        description="FAISS ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì „ë¬¸ ì¤‘ ëª¨ë¥´ëŠ” ë‹¨ì–´ë¥¼ ê²€ìƒ‰í•´ ë‹¨ì–´ ì •ì˜ë¥¼ ë°˜í™˜í•˜ëŠ” íˆ´. ì…ë ¥ì€ JSON ë°°ì—´ í˜•ì‹ì˜ ë¬¸ìì—´ ë˜ëŠ” ë‹¨ì¼ ìš©ì–´ ë¬¸ìì—´.",
        args_schema=RetrievalInput
    )

    tools = [retrieval_tool]

    agent = create_react_agent(model, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, max_iterations=5, max_execution_time=1000, handle_parsing_errors=True)

    return agent_executor


