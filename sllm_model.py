import os
import json
# import platform
from pathlib import Path
# from dataclasses import dataclass
from typing import List, Dict, Any, Tuple
from pydantic import BaseModel, Field
import traceback

# from langchain.tools import tool
from langchain.agents import create_react_agent, AgentExecutor, Tool
from langchain.prompts import PromptTemplate
from datetime import datetime

from main_model import load_model_q, load_faiss_db, preprocess_transcript

# ===== ê¸°ë³¸ì„¤ì • =====
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"

db_path = "./faiss_db_merged"
vector_store, embedding_model = load_faiss_db(db_path)

PROMPT_DIR = Path(__file__).parent / "prompts"
SYSTEM_PROMPT = (PROMPT_DIR / "system.txt").read_text(encoding="utf-8").strip()
PROMPTS = {
    "summarizer": (PROMPT_DIR / "summarizer.txt").read_text(encoding="utf-8").strip(),
    "task_extractor": (PROMPT_DIR / "extract_tasks.txt").read_text(encoding="utf-8").strip(),
}
summarizer_prompt = PROMPTS["summarizer"]
task_prompt = PROMPTS["task_extractor"]


# ===== ì²­í¬ ë¶„í•  (1500ì ê¸°ì¤€ ì• ë’¤ ë¬¸ì¥) =====
def chunk_transcript(transcript: str, max_tokens: int = 1500) -> List[str]:
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° ê¸°ì¤€)
    sentences = []
    current = ""
    for char in transcript:
        current += char
        if char in ['.', '!', '?', '\n'] or (char == 'ë‹¤' and len(current) > 20):
            if current.strip():
                sentences.append(current.strip())
            current = ""
    if current.strip():
        sentences.append(current.strip())

    # ì²­í¬ ìƒì„±
    chunks = []
    current_chunk = ""
    current_length = 0

    for sentence in sentences:
        sentence_length = len(sentence)

        if current_length + sentence_length > max_tokens and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = sentence
            current_length = sentence_length
        else:
            current_chunk += " " + sentence
            current_length += sentence_length

    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    return chunks


# ===== ì²­í¬ ë³„ ì²˜ë¦¬ ë° ì „ì²´ ìš”ì•½/íƒœìŠ¤í¬ ì¶”ì¶œ =====
def process_transcript_with_chunks(transcript: str, domain) -> dict:

    user_domain = domain
    if not user_domain :
        domain_filter = None
    else:
        domain_filter = user_domain

    # ì „ë¬¸ ë¬¸ì¥ìœ¼ë¡œ ì „ì²˜ë¦¬
    transcript = preprocess_transcript(transcript)

    # ì—ì´ì „íŠ¸ ë¹Œë“œ
    agent = build_agent(model=load_model_q(), vector_store=vector_store, domain=domain_filter)

    # ê¸´ íšŒì˜ë¡ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì²­í¬ í¬ê¸° ì„¤ì •
    max_chunk_len = 3000

    # ì „ë¬¸ ê¸¸ì´ í™•ì¸ í›„ ì²­í¬ ë¶„í•  ì—¬ë¶€ ê²°ì •
    if len(transcript) > max_chunk_len:
        chunks = chunk_transcript(transcript, max_chunk_len)
        print(f"âš ï¸ ì „ë¬¸ì´ ê¹ë‹ˆë‹¤! {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n")

        chunk_results = []
        for i, chunk in enumerate(chunks, 1):
            print(f"\n[ì²­í¬ {i}/{len(chunks)}] ì²˜ë¦¬ ì¤‘... (ê¸¸ì´: {len(chunk)} ê¸€ì)")
            try:
                # ê° ì²­í¬ë³„ë¡œ ì „ë¬¸ ìš©ì–´ ê²€ìƒ‰ (ì„ íƒì )
                result = agent.invoke({"input": f"ë‹¤ìŒì€ íšŒì˜ë¡ì˜ ì¼ë¶€ì…ë‹ˆë‹¤. ì´í•´í•˜ê¸° ì–´ë ¤ìš´ ì „ë¬¸ ìš©ì–´ê°€ ìˆìœ¼ë©´ ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”:\n\n{chunk}"})
                chunk_results.append({"chunk_index": i, "chunk_length": len(chunk), "result": result.get("output", "")})
                print(f"[ì²­í¬ {i}] ì²˜ë¦¬ ì™„ë£Œ")
            except Exception as e:
                print(f"[ì²­í¬ {i}] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                chunk_results.append({
                    "chunk_index": i,
                    "chunk_length": len(chunk),
                    "error": str(e)
                })

        # ì²­í¬ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì „ì²´ ì „ë¬¸ì„ ìš”ì•½ë³¸ìœ¼ë¡œ ì¶•ì•½
        if len(chunks) > 5:
            print(f"\nâš ï¸ ì²­í¬ê°€ {len(chunks)}ê°œë¡œ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤. ì „ì²´ ìš”ì•½/íƒœìŠ¤í¬ ì¶”ì¶œ ì‹œ ì²­í¬ë³„ ìš”ì•½ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            # ì²­í¬ë³„ ê²°ê³¼ë¥¼ í•©ì³ì„œ ì¶•ì•½ëœ ì „ë¬¸ ìƒì„±
            condensed_transcript = "\n\n".join([
                f"[ì²­í¬ {cr['chunk_index']}] {cr.get('result', '')[:500]}..."
                for cr in chunk_results if 'result' in cr
            ])
            use_full_transcript = False
        else:
            condensed_transcript = transcript
            use_full_transcript = True
    else:
        # ì „ë¬¸ ê¸¸ì´ ì ì ˆ ì‹œ - í’€ë¡œ ì§„í–‰
        print("âœ… ì „ë¬¸ ê¸¸ì´ê°€ ì ì ˆí•©ë‹ˆë‹¤. ì²­í¬ ë¶„í•  ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.\n")
        chunk_results = []
        condensed_transcript = transcript
        use_full_transcript = True

    # ì „ì²´ ì „ë¬¸ ê¸°ë°˜ìœ¼ë¡œ ì„¸ë¶€ ì•ˆê±´, ì•ˆê±´ë³„ ìš”ì•½ ì¶”ì¶œ
    print(f"\n{'='*60}")
    print("ì „ì²´ ì „ë¬¸ ê¸°ë°˜ ì•ˆê±´/ìš”ì•½ ì¶”ì¶œ ì¤‘...")
    print(f"{'='*60}\n")

# ===== ì•ˆê±´/ìš”ì•½ ì¶”ì¶œ =====
    try:
        transcript_for_analysis = condensed_transcript if not use_full_transcript else transcript
        filled_summary_prompt = f"""
            ë‹¤ìŒ íšŒì˜ ì „ë¬¸ì„ ë¶„ì„í•˜ì„¸ìš”.
            {summarizer_prompt.format(transcript=transcript_for_analysis)}
            
            CRITICAL INSTRUCTIONS:
            1. ONLY use information EXPLICITLY mentioned in the transcript above
            2. DO NOT invent or hallucinate any information
            3. If you need to look up unknown terms, use the retrieval tool with at most 5 SHORT terms
            4. Your final answer MUST start with "Final Answer:" followed by the JSON
            5. DO NOT include any explanations before or after the JSON"""

        summary_result = agent.invoke({"input": filled_summary_prompt})
        full_summary = summary_result.get("output", "")

        if "Agent stopped" in full_summary or "iteration limit" in full_summary:   # Agentê°€ iteration limitì— ë„ë‹¬í•œ ê²½ìš° intermediate_stepsì—ì„œ ê²°ê³¼ ì¶”ì¶œ
            print("âš ï¸ Agent iteration limit ë„ë‹¬, ì¤‘ê°„ ê²°ê³¼ ì¶”ì¶œ ì‹œë„...")
            intermediate_steps = summary_result.get("intermediate_steps", [])
            
            # # if intermediate_steps:
            # #     # ë§ˆì§€ë§‰ agent ì¶œë ¥ì—ì„œ JSON ì¶”ì¶œ

            # # ëª¨ë“  step ì¶œë ¥ í™•ì¸ (ë””ë²„ê¹…)
            # print(f"   ì´ {len(intermediate_steps)}ê°œì˜ intermediate steps ë°œê²¬")

            # ëª¨ë“  LLM ì¶œë ¥ì—ì„œ JSON ì°¾ê¸°
            for i, step in enumerate(intermediate_steps):
                if len(step) >= 1:
                    # stepì€ (AgentAction, observation) íŠœí”Œ
                    agent_action = step[0] if len(step) > 0 else None

                    # AgentActionì˜ logì—ì„œ JSON ì¶”ì¶œ
                    if hasattr(agent_action, 'log'):
                        log_text = str(agent_action.log)
                        if '{' in log_text and '"agendas"' in log_text:
                            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                            start_idx = log_text.find('{')
                            end_idx = log_text.rfind('}') + 1
                            if start_idx != -1 and end_idx > start_idx:
                                json_candidate = log_text[start_idx:end_idx]
                                # ìœ íš¨ì„± ê²€ì¦
                                try:
                                    import json as json_module
                                    parsed = json_module.loads(json_candidate)
                                    if 'agendas' in parsed:
                                        full_summary = json_candidate
                                        print(f"âœ… Step {i}ì—ì„œ ìœ íš¨í•œ JSON ì¶”ì¶œ ì„±ê³µ!")
                                        break
                                except:
                                    continue

            # ìœ„ ë°©ë²•ì´ ì‹¤íŒ¨í•˜ë©´ ë§ˆì§€ë§‰ ì¶œë ¥ì—ì„œ ì¶”ì¶œ
            if "Agent stopped" in full_summary:
                for step in reversed(intermediate_steps):
                    if len(step) > 0 and hasattr(step[0], 'log'):
                        log_text = str(step[0].log)
                        if '{' in log_text and '}' in log_text:
                            full_summary = log_text.strip()
                            print(f"âœ… í´ë°±: ë§ˆì§€ë§‰ stepì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ")
                # for step in reversed(intermediate_steps):
                    # if len(step) > 1 and hasattr(step[1], '__str__'):
                    #     potential_output = str(step[1])
                    #     if '{' in potential_output and '}' in potential_output:
                    #         full_summary = potential_output
                    #         print(f"âœ… ì¤‘ê°„ ê²°ê³¼ ì¶”ì¶œ ì„±ê³µ (ê¸¸ì´: {len(full_summary)}ì)")
                    #         break

        print("âœ… ì•ˆê±´/ìš”ì•½ ì¶”ì¶œ ì™„ë£Œ\n")
    except Exception as e:
        print(f"âŒ ì•ˆê±´/ìš”ì•½ ì¶”ì¶œ ì‹¤íŒ¨: {e}\n")
        full_summary = {"error": str(e)}

    print(f"\n{'='*60}")
    print("ì „ì²´ ì „ë¬¸ ê¸°ë°˜ íƒœìŠ¤í¬ ì¶”ì¶œ ì¤‘...")
    print(f"{'='*60}\n")

# ===== íƒœìŠ¤í¬ ì¶”ì¶œ =====   
    try:
        transcript_for_analysis = condensed_transcript if not use_full_transcript else transcript
        current_date=datetime.now().date().isoformat()
        date_obj = datetime.strptime(current_date, "%Y-%m-%d")
        weekdays = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"]
        current_weekday=weekdays[date_obj.weekday()]
        
        filled_task_prompt = f"""
            ë‹¤ìŒ íšŒì˜ ì „ë¬¸ì„ ë¶„ì„í•˜ì„¸ìš”.
    
            {task_prompt.format(transcript=transcript_for_analysis, current_date=datetime.now().date().isoformat(), current_weekday=current_weekday)}
            
            CRITICAL INSTRUCTIONS:
            1. ONLY extract tasks and assignees that are EXPLICITLY mentioned in the transcript
            2. DO NOT invent names like "ê¹€ì˜í¬" - ONLY use names that appear in the transcript
            3. If an assignee is not clearly stated, use null
            4. If you need to look up unknown terms, use the retrieval tool with at most 5 SHORT terms
            5. Your final answer MUST start with "Final Answer:" followed by the JSON
            6. DO NOT include any explanations before or after the JSON"""

        task_result = agent.invoke({"input": filled_task_prompt})
        full_tasks = task_result.get("output", "")

        # Agentê°€ iteration limitì— ë„ë‹¬í•œ ê²½ìš° intermediate_stepsì—ì„œ ê²°ê³¼ ì¶”ì¶œ
        if "Agent stopped" in full_tasks or "iteration limit" in full_tasks:
            print("âš ï¸ Agent iteration limit ë„ë‹¬, ì¤‘ê°„ ê²°ê³¼ ì¶”ì¶œ ì‹œë„...")
            intermediate_steps = task_result.get("intermediate_steps", [])
            if intermediate_steps:
                # ë§ˆì§€ë§‰ agent ì¶œë ¥ì—ì„œ JSON ì¶”ì¶œ
                for step in reversed(intermediate_steps):
                    if len(step) > 1 and hasattr(step[1], '__str__'):
                        potential_output = str(step[1])
                        if '{' in potential_output and '}' in potential_output:
                            full_tasks = potential_output
                            print(f"âœ… ì¤‘ê°„ ê²°ê³¼ ì¶”ì¶œ ì„±ê³µ (ê¸¸ì´: {len(full_tasks)}ì)")
                            break

        print("âœ… íƒœìŠ¤í¬ ì¶”ì¶œ ì™„ë£Œ\n")
    except Exception as e:
        print(f"âŒ íƒœìŠ¤í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}\n")
        full_tasks = {"error": str(e)}

    # return {"chunk_results": chunk_results, "full_summary": full_summary, "full_tasks": full_tasks}
    return {"full_summary": full_summary, "full_tasks": full_tasks}


# ===== agent!!! =====
def build_agent(model, vector_store, domain) :

    search_kwargs = {"k": 20}
    if domain:
        search_kwargs["filter"] = {"domain": domain}

    retriever = vector_store.as_retriever(search_kwargs=search_kwargs)

    template = '''
    AI meeting analyzer for {domain}.

    Tools: {tools}  

    FORMAT (MANDATORY):
    Thought: [reasoning]
    Action: [{tool_names}] or "None"
    Action Input: [input] or "N/A"
    Observation: [result]
    ...(repeat if needed)
    Thought: I now know the final answer
    Final Answer: [Korean JSON] 

    RULES:
    1. ALWAYS end with "Final Answer:" - NO exceptions
    2. Use retrieval ONLY for unknown terms (max 5, under 50 chars each): ["term1","term2"]
    3. Use ONLY explicit transcript info - NO hallucinations
    4. Keep it concise  

    Input: {input}
    Thought:{agent_scratchpad}'''

    prompt = PromptTemplate.from_template(template).partial(domain=domain)

    # retrieval tool
    def retrieval_func(term_list: str) -> dict:
        """
        FAISS ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì „ë¬¸ ì¤‘ ëª¨ë¥´ëŠ” ë‹¨ì–´ë¥¼ ê²€ìƒ‰í•´ ë‹¨ì–´ ì •ì˜ë¥¼ ë°˜í™˜í•˜ëŠ” íˆ´.
        Args: term_list: ê²€ìƒ‰í•  ìš©ì–´ë“¤ (ë‹¨ì¼ ë¬¸ìì—´ ë˜ëŠ” JSON í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´)
             ì˜ˆ: "API" ë˜ëŠ” '["API", "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤"]'
        Returns: ê²€ìƒ‰ëœ ìš©ì–´ë“¤ì˜ ì •ì˜ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        """
        print('='*60)
        print('ğŸ” retrieval_func í˜¸ì¶œë¨')
        print('ì…ë ¥ê°’:', term_list)
        print('ì…ë ¥ íƒ€ì…:', type(term_list))
        print("í˜„ì¬ domain í•„í„° = ", domain)
        print('='*60)

        # ìµœëŒ€ ê²€ìƒ‰ì–´ ê¸¸ì´ ì œí•œ (ì„ë² ë”© ëª¨ë¸ì˜ í† í° ì œí•œ ê³ ë ¤)
        MAX_TERM_LENGTH = 100  # í•œ ìš©ì–´ë‹¹ ìµœëŒ€ 100ì

        # ì…ë ¥ ì •ê·œí™”
        try:
            if isinstance(term_list, str):
                term_list = term_list.strip()
                if term_list.startswith('[') and term_list.endswith(']'):
                    try:
                        term_list_local = json.loads(term_list)
                        if not isinstance(term_list_local, list):
                            term_list_local = [str(term_list_local)]
                    except json.JSONDecodeError as je:
                        print(f'âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {je}, ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì‚¬ìš©')
                        # ëŒ€ê´„í˜¸ ì œê±°í•˜ê³  ì‰¼í‘œë¡œ split
                        term_list_local = [t.strip().strip('"\'') for t in term_list.strip('[]').split(',')]
                elif ',' in term_list:
                    term_list_local = [t.strip().strip('"\'') for t in term_list.split(',')]
                else:
                    term_list_local = [term_list]
            elif isinstance(term_list, list):
                term_list_local = term_list

            elif isinstance(term_list, tuple):
                term_list_local = list(term_list)

            else:
                print(f'âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…: {type(term_list)}, ë¬¸ìì—´ë¡œ ë³€í™˜')
                term_list_local = [str(term_list)]

            print('íŒŒì‹±ëœ term_list_local:', term_list_local)
            term_list_local = [str(term).strip().strip('"\'') for term in term_list_local if term]

            # ë„ˆë¬´ ê¸´ ê²€ìƒ‰ì–´ í•„í„°ë§ ë° ê²½ê³ 
            filtered_terms = []
            for t in term_list_local:
                if t:
                    if len(t) > MAX_TERM_LENGTH:
                        print(f'âš ï¸ ê²€ìƒ‰ì–´ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ (ê¸¸ì´: {len(t)}). ì• {MAX_TERM_LENGTH}ìë§Œ ì‚¬ìš©: "{t[:MAX_TERM_LENGTH]}..."')
                        filtered_terms.append(t[:MAX_TERM_LENGTH])
                    else:
                        filtered_terms.append(t)

            term_list_local = filtered_terms
            print('ìµœì¢… ì •ë¦¬ëœ term_list_local:', term_list_local)

        except Exception as parse_error:
            print(f'âŒ ì…ë ¥ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {type(parse_error).__name__}: {parse_error}')
            print(f'ì›ë³¸ ì…ë ¥ì„ ë‹¨ì¼ í•­ëª©ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤: {repr(term_list)}')
            term_list_local = [str(term_list).strip()]

        if not term_list_local:
            print('ê²€ìƒ‰í•  ìš©ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.')
            return {"output": json.dumps({"definitions": {}}, ensure_ascii=False)}

        # ê°œë³„ ê²€ìƒ‰ ì²˜ë¦¬
        all_docs_list = []
        k = search_kwargs.get("k", 20)
        filter_dict = search_kwargs.get("filter", None)

        for idx, term in enumerate(term_list_local):
            try:
                # termì„ ëª…ì‹œì ìœ¼ë¡œ ë¬¸ìì—´ë¡œ ë³€í™˜
                term_str = str(term).strip()
                print(f'\n[{idx+1}/{len(term_list_local)}] ê²€ìƒ‰ ì‹œì‘')
                print(f'  ì›ë³¸ term: {repr(term)} (íƒ€ì…: {type(term).__name__})')
                print(f'  ë³€í™˜ term_str: {repr(term_str)} (íƒ€ì…: {type(term_str).__name__})')

                if not term_str:
                    print(f'âš ï¸ ë¹ˆ ê²€ìƒ‰ì–´ ê±´ë„ˆëœ€')
                    all_docs_list.append([])
                    continue

                print(f'  ğŸ” FAISS ê²€ìƒ‰ ì‹¤í–‰: "{term_str}"')
                print(f'  ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: k={k}, filter={filter_dict}')

                # query íƒ€ì… ì¬í™•ì¸
                if not isinstance(term_str, str):
                    raise TypeError(f"query must be str, got {type(term_str).__name__}")

                if len(term_str) == 0:
                    print(f'âš ï¸ ë¹ˆ ë¬¸ìì—´, ê±´ë„ˆëœ€')
                    all_docs_list.append([])
                    continue

                # ì¶”ê°€ ì•ˆì „ì¥ì¹˜: ê²€ìƒ‰ì–´ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ë‹¤ì‹œ í•œë²ˆ í™•ì¸
                if len(term_str) > MAX_TERM_LENGTH:
                    print(f'âš ï¸ ê²€ìƒ‰ì–´ ì¬í™•ì¸: ê¸¸ì´ {len(term_str)} > {MAX_TERM_LENGTH}, ì˜ë¼ëƒ„')
                    term_str = term_str[:MAX_TERM_LENGTH]

                # FAISS ê²€ìƒ‰ ì‹¤í–‰
                try:
                    docs = vector_store.similarity_search(
                        term_str,  # í‚¤ì›Œë“œ ì¸ì ëŒ€ì‹  ìœ„ì¹˜ ì¸ì ì‚¬ìš©
                        k=k,
                        filter=filter_dict
                    )
                    all_docs_list.append(docs)
                    print(f'  âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ ë°œê²¬')
                except Exception as search_error:
                    print(f'  âŒ FAISS ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {type(search_error).__name__}: {search_error}')
                    print(f'     ê²€ìƒ‰ì–´: "{term_str[:50]}..." (ê¸¸ì´: {len(term_str)})')
                    all_docs_list.append([])
                    continue
            
            except Exception as e:
                print(f'  âŒ ê²€ìƒ‰ ì‹¤íŒ¨!')
                print(f'     ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}')
                print(f'     ì˜¤ë¥˜ ë©”ì‹œì§€: {e}')
                print(f'     ë¬¸ì œëœ term: {repr(term)}')
                
                traceback.print_exc()
                all_docs_list.append([])

        definitions = {}
        for term, docs in zip(term_list_local, all_docs_list):
            if not docs:
                print(f'@#$@#$@${term}ì— ëŒ€í•œ ìš©ì–´ ëª»ì°¾ìŒ@#$#@$@#')
                continue

            defs = []
            for d in docs[:2]:
                ans = d.metadata.get("answer") or d.page_content
                defs.append(ans.strip())

            definitions[term] = "\n\n".join(defs)

        print('ì—¬ê¸°ë‹¤ ì—¬ê¸°!!! ë‹¨ì–´ ì •ì˜ : ', definitions)
        retrieval_result = {"output": json.dumps({"definitions": definitions}, ensure_ascii=False)}

        return retrieval_result

    # Tool ê°ì²´ ì§ì ‘ ìƒì„±
    class RetrievalInput(BaseModel):
        term_list: str = Field(description="ê²€ìƒ‰í•  ìš©ì–´ë“¤ (ë‹¨ì¼ ë¬¸ìì—´ ë˜ëŠ” JSON ë°°ì—´ ë¬¸ìì—´)")

    retrieval_tool = Tool(
        name="retrieval",
        func=retrieval_func,
        description="FAISS ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì „ë¬¸ ì¤‘ ëª¨ë¥´ëŠ” ë‹¨ì–´ë¥¼ ê²€ìƒ‰í•´ ë‹¨ì–´ ì •ì˜ë¥¼ ë°˜í™˜í•˜ëŠ” íˆ´. ì…ë ¥ì€ JSON ë°°ì—´ í˜•ì‹ì˜ ë¬¸ìì—´ ë˜ëŠ” ë‹¨ì¼ ìš©ì–´ ë¬¸ìì—´.",
        args_schema=RetrievalInput
    )

    tools = [retrieval_tool]

    agent = create_react_agent(model, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, max_iterations=20, max_execution_time=3000, handle_parsing_errors=True)

    return agent_executor


