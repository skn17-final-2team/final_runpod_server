# pip install hf_transfer accelerate peft
# pip install langchain==0.3.27 langchain-core==0.3.76 langchain-community==0.3.30 langchain-text-splitters==0.3.11 langchain-huggingface==0.3.1 langchain-ollama==0.3.10
# pip install torch torchvision torchaudio transformers sentence-transformers faiss-cpu

import os, time, torch, platform, re, json 
from dotenv import load_dotenv
from huggingface_hub import login
from pathlib import Path

from peft import PeftModel
from langchain.vectorstores import FAISS
from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace

from langchain.prompts import ChatPromptTemplate
from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser

from langchain.tools import tool
from langchain.agents import create_react_agent, AgentExecutor
from langchain.prompts import MessagesPlaceholder

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"

PROMPT_DIR = Path(__file__).parent / "prompts"
SYSTEM_PROMPT = (PROMPT_DIR / "system.txt").read_text(encoding="utf-8").strip()
PROMPTS = {
    "summarizer": (PROMPT_DIR / "summarizer.txt").read_text(encoding="utf-8").strip(),
    "task_extractor": (PROMPT_DIR / "extract_tasks.txt").read_text(encoding="utf-8").strip(),
}

base_model_name = "Qwen/Qwen2.5-1.5B-Instruct"
ft_model_name = "CHOROROK/Qwen2.5_1.5B_trained_model_v3"

# ===== ì´ìŠ¤ì¼€ì´íŠ¸ ====
def escape_curly(text: str) -> str:
    return text.replace("{", "{{").replace("}", "}}")

# ===== ë²¡í„° DB ë¡œë“œ =====
def load_faiss_db(db_path: str):
    embedding_model = HuggingFaceEmbeddings(model_name="dragonkue/snowflake-arctic-embed-l-v2.0-ko")
    vector_store = FAISS.load_local(db_path, embedding_model, allow_dangerous_deserialization=True)
    print("ğŸ”µ FAISS DB ë¡œë“œ ì™„ë£Œ!\n")
    return vector_store, embedding_model


# ===== ëª¨ë¸ ë¡œë“œ =====
def load_model_q(model_name, adapter_name: str | None = None):
    if platform.system() == "Windows":
        print("âš  Windowsì—ì„œëŠ” 4bit ë¶ˆê°€ â†’ FP16ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.")
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map='auto'
        )
    else:
        print("ğŸ”µ Linux/RunPod í™˜ê²½: 4bit ì—†ì´ bf16ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.")
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.bfloat16,   # ì•ˆ ë˜ë©´ torch.float16 ë¡œ ë°”ê¿”ë„ ë¨
            device_map="auto",
        )

    if adapter_name:
        print(f"ğŸ”µ LoRA/PEFT ì–´ëŒ‘í„° ë¡œë“œ: {adapter_name}")
        model = PeftModel.from_pretrained(base_model, adapter_name)
    else:
        model = base_model

    text_gen_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        return_full_text=False, 
        max_new_tokens = 2048, 
        temperature=0.2,
        top_p=0.9
    )

    llm = HuggingFacePipeline(pipeline=text_gen_pipe)
    return llm


# ===== ë„ë©”ì¸ í•„í„° =====
def make_filter(filter: dict):
    if any(filter.values()):
        main_filter = filter.copy()
    else:
        main_filter = None
    return main_filter


# ===== RAG ë‹¨ì–´ ì¶”ì¶œ â€“ í˜„ì¬ ì•ˆì”€ =====
def make_rag_result(model, meeting_text):
    instruction = """
    ë‹¹ì‹ ì€ íšŒì˜ë¡ ì „ë¬¸ì„ ë¶„ì„í•˜ëŠ” AIì…ë‹ˆë‹¤. ì˜ë¯¸ê°€ ëª¨í˜¸í•œ ë‹¨ì–´ë¥¼ ëª¨ë‘ ì¤‘ë³µì—†ì´ ì¶”ì¶œí•˜ì„¸ìš”.
    - ì˜ë¯¸ê°€ ëª¨í˜¸í•œ ìš©ì–´ëŠ” ì ˆëŒ€ ì¶”ì¸¡í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì¶”ì¶œ
    - ì¼ë°˜ ì¸ì‚¬, ì¡ë‹´ì€ ì œì™¸
    - ì¶œë ¥ì€ ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë‹¨ì–´ ëª©ë¡ìœ¼ë¡œ í•´ì£¼ì„¸ìš”.
    """
    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(instruction),
        HumanMessagePromptTemplate.from_template("íšŒì˜ë¡: {text}")
    ])

    formatted_prompt = prompt.format(text=meeting_text)
    output = model(formatted_prompt, temperature=0.2, top_p=0.9)

    rag_word_list = [w.strip() for w in re.split(r'[, \n]+', output) if w.strip()]
    print("ğŸ”¹ ëª¨ë¥´ëŠ” ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸:", rag_word_list)
    return rag_word_list


# ===== ìš©ì–´ ì¶”ì¶œìš© ì²´ì¸ =====
def build_term_extractor_chain(llm: ChatHuggingFace):
    """íšŒì˜ë¡ì—ì„œ ëª¨í˜¸í•œ/í•µì‹¬ ìš©ì–´ë¥¼ ì½¤ë§ˆë¡œ ì¶”ì¶œí•˜ëŠ” ì²´ì¸."""
    instruction = """
    ë‹¹ì‹ ì€ IT íšŒì˜ë¡ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ íšŒì˜ë¡ì—ì„œ 'ì •ì˜ê°€ í•„ìš”í•´ ë³´ì´ëŠ” ìš©ì–´'ë¥¼ 5~15ê°œ ì •ë„ ë½‘ì•„ì£¼ì„¸ìš”.

    ê¸°ì¤€:
    - ì„œë¹„ìŠ¤/ê¸°ëŠ¥ ì´ë¦„, ê¸°ìˆ  ìš©ì–´, ì•½ì–´, ì§€í‘œ/ì§€ìˆ˜, ì •ì±…/ê·œì¹™ ì´ë¦„ ë“±
    - ì¼ë°˜ì ì¸ ì¼ìƒì–´(ì•ˆë…•í•˜ì„¸ìš”, ë„¤, ì¢‹ì•„ìš” ë“±)ëŠ” ì œì™¸
    - ì´ë¯¸ ë„ˆë¬´ ëª…í™•í•œ ë‹¨ì–´(ì˜ˆ: ë¡œê·¸ì¸, ë²„íŠ¼)ë„ ì›¬ë§Œí•˜ë©´ ì œì™¸
    - ì¶œë ¥ì€ ì˜¤ì§ ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ìš©ì–´ ë¦¬ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ì„¸ìš”. ì˜ˆ)
      íšŒì›ê°€ì… SSO, ì‘ì—… ë³´ë“œ CRUD, RICE ìŠ¤ì½”ì–´, CI/CD í’ˆì§ˆ ê²Œì´íŠ¸
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessagePromptTemplate.from_template(instruction),
            HumanMessagePromptTemplate.from_template("íšŒì˜ë¡ ì „ë¬¸:\n{text}"),
        ]
    )

    parser = StrOutputParser()
    chain = prompt | llm | parser
    return chain


# ===== definitions =====
class DefinitionAgent:

    def __init__(self, llm: ChatHuggingFace, vector_store: FAISS, default_domain: str = "IT"):
        self.llm = llm
        self.default_domain = default_domain

        # ë²¡í„°ìŠ¤í† ì–´ retriever ì¤€ë¹„
        self.retriever = vector_store.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                "score_threshold": 0.65,
                "filter": {"domain": default_domain},
            },
        )

        # ìš©ì–´ ì¶”ì¶œ ì²´ì¸
        self.term_chain = build_term_extractor_chain(llm)

    def invoke(self, inputs: dict):
        """LangChain AgentExecutorì™€ ë§ì¶”ê¸° ìœ„í•´ .invoke(dict)ë¥¼ ì œê³µ."""
        text = inputs.get("input", "")
        if not text:
            return {"output": json.dumps({"definitions": {}}, ensure_ascii=False)}

        # ì¶”ì¶œ
        terms_text = self.term_chain.invoke({"text": str(text or "")})
        term_list = [t.strip() for t in re.split(r"[,;\n]+", terms_text) if t.strip()]

        print("ğŸ”¹ğŸ”¹ğŸ”¹type of text:", type(text), text)
        print("ğŸ”¹ ì—ì´ì „íŠ¸ ì¶”ì¶œ ìš©ì–´:", term_list)

        # term ì •ì˜ ê²€ìƒ‰
        definitions = {}
        for term in term_list:
            docs = self.retriever.invoke(term)
            if not docs:
                # ëª» ì°¾ì€ ìš©ì–´ëŠ” íŒ¨ã…¡
                continue

            # ê°€ì¥ ê´€ë ¨ë„ ë†’ì€ ë¬¸ì„œ 1~2ê°œë¥¼ í•©ì³ì„œ ì •ì˜ë¡œ ì‚¬ìš©
            defs = []
            for d in docs[:2]:
                ans = d.metadata.get("answer") or d.page_content
                defs.append(ans.strip())

            definitions[term] = "\n\n".join(defs)

        # 3) JSON ë¬¸ìì—´ë¡œ ë°˜í™˜
        return {
            "output": json.dumps({"definitions": definitions}, ensure_ascii=False)
        }


def build_agent(llm, vector_store, default_domain="IT"):
    return DefinitionAgent(llm, vector_store, default_domain)


def make_chain(model):
    summarizer_prompt = PROMPTS["summarizer"]
    task_prompt = PROMPTS["task_extractor"]  
    
    safe_summarizer = escape_curly(PROMPTS["summarizer"])
    safe_task_prompt = escape_curly(PROMPTS["task_extractor"])
 
    instruction = ("""
    [SYSTEM_PROMPT]
        [ì•ˆê±´ / ìš”ì•½ ì§€ì¹¨]
        ë‹¤ìŒ ë‚´ìš©ì„ íšŒì˜ë¡ ì•ˆê±´ ì¶”ì¶œ ë° ìš”ì•½ íŒŒíŠ¸ì— ì ìš©í•˜ë¼:
        {{safe_summarizer}}

        [íƒœìŠ¤í¬ ì¶”ì¶œ ì§€ì¹¨]
        ë‹¤ìŒ ë‚´ìš©ì„ tasks ì¶”ì¶œ íŒŒíŠ¸ì— ì ìš©í•˜ë¼:
        {{safe_task_prompt}}

    [ê³µí†µ ì¶œë ¥ ê·œì¹™]
    - ìµœì¢… ì¶œë ¥ì€ ë°˜ë“œì‹œ í•˜ë‚˜ì˜ JSON ë¬¸ìì—´ë§Œ ë°˜í™˜í•œë‹¤.
    - ë¶ˆí•„ìš”í•œ ìì—°ì–´ ì„¤ëª…, ì•ë’¤ ì¸ì‚¬ë§, ì½”ë“œ ë¸”ë¡ ë§ˆí¬ë‹¤ìš´(````json` ë“±)ì€ ì ˆëŒ€ ë„£ì§€ ì•ŠëŠ”ë‹¤.
    - keysë¥¼ ì¤‘ë³µ ì •ì˜í•˜ì§€ ì•ŠëŠ”ë‹¤. (ì˜ˆ: "tasks"ë¥¼ ë‘ ë²ˆ ì“°ì§€ ë§ ê²ƒ)
    - definitions(ìš©ì–´ ì •ì˜)ëŠ” ì°¸ê³ ë§Œ í•˜ê³ , summary/tasks/issuesì— ê·¸ëŒ€ë¡œ ì¥ë¬¸ ë³µë¶™í•˜ì§€ ë§ ê²ƒ.

    ì¶œë ¥ ìŠ¤í‚¤ë§ˆ(ì˜ˆì‹œ):

    {{
      "agendas": [
        {{
        "agenda_1": {{
        "who": "...",
        "what": "...",
        "when": "...",
        "where": "...",
        "why": "...",
        "how": "...",
        "how_much": "...",
        "how_long": "..."
        }},
        "agenda_2": {{
          "who": "...",
          "what": "...",
          "when": "...",
          "where": "...",
          "why": "...",
          "how": "...",
          "how_much": "...",
          "how_long": "..."
        }}],
      "tasks": [
        {{
          "owner": "ì´ë¦„ ë˜ëŠ” ì—­í• ",
          "task": "í•´ì•¼ í•  ì¼",
          "due": "YYYY-MM-DD ë˜ëŠ” 'TBD' í˜¹ì€ ë¹ˆ ë¬¸ìì—´"
        }}
      ]
    }}
    """)

    # instruction = _escape_curly(instruction)
    parser = StrOutputParser()
    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(instruction),
        HumanMessagePromptTemplate.from_template(
            "ì‚¬ìš©ì íšŒì˜ë¡: {text}\n\n"
            "ì°¸ê³  ë¬¸ì„œ:\n{rag_result_text}\n\n"
            "ìœ„ System í”„ë¡¬í”„íŠ¸ì™€ ê° ì—­í• ë³„ í”„ë¡¬í”„íŠ¸ ì§€ì¹¨ì„ ëª¨ë‘ ë°˜ì˜í•˜ì—¬,\n"
            "ë°˜ë“œì‹œ í•˜ë‚˜ì˜ JSONë§Œ ìƒì„±í•˜ì„¸ìš”."
        )
    ])
    chain = prompt | model | parser
    return chain

def run_inference_model(transcript: str):
    load_dotenv()
    db_path = './faiss_db/rag_it_tta'
    HF_TOKEN = os.getenv('HF_TOKEN')

    if HF_TOKEN:
        login(token=HF_TOKEN, add_to_git_credential=False)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("Device set to:", device)

    vector_store, embedding_model = load_faiss_db(db_path)
    base_model = load_model_q(base_model_name)
    ft_model = load_model_q(base_model_name, adapter_name = ft_model_name)

    agent = build_agent(base_model, vector_store, default_domain='IT')

    chain = make_chain(ft_model)
    print("íšŒì˜ë¡ ì „ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”! ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥\n")

    while True:
        query = input("ì „ë¬¸: ")
        if query.lower() in ["exit", "quit"]:
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        print("\n--- ğŸ” ì—ì´ì „íŠ¸: ë‹¨ì–´ ì •ì˜ ì¶”ì¶œì¤‘ ---")
        agent_result = agent.invoke({"input": query})
        rag_result_text = agent_result["output"]
        print(" ğŸ” ì—ì´ì „íŠ¸ definitions:", rag_result_text)

        print("\n--- ğŸ¤– íŒŒíŠœ ëª¨ë¸: ìš”ì•½ ìƒì„±ì¤‘ ---")
        result = chain.invoke({
            "text": query,
            "rag_result_text": rag_result_text
        })

        # AgentExecutorëŠ” ë³´í†µ {"output": "...", ...} í˜•íƒœ ë°˜í™˜
        print("\nëª¨ë¸ ì‘ë‹µ(JSON):\n", result)

    return {"success": True, "data": {"summary": result['agedas'], "tasks": result['tasks'],}}


if __name__ == "__main__":
    q = input('ì „ë¬¸: ')
    result_final = run_inference_model(q)
    print("\nëª¨ë¸ ì‘ë‹µ(JSON):\n", result_final)